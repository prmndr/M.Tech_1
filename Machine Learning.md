# Introduction to Probability and Statistics - UNIT_1

## Topic Importance Classification

### **HIGHEST PRIORITY (Essential for ML)**
1. **Gaussian/Normal Distribution and their PDF's and CDF's** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
   - Foundation for many ML algorithms (Linear Regression, Naive Bayes, etc.)
   - Central Limit Theorem applications

2. **Population and Sample** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
   - Fundamental concept for data science
   - Essential for understanding bias and generalization

3. **Hypothesis Testing Methodology** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
   - Core statistical inference concept
   - Critical for model validation and A/B testing

4. **p-value and Null Hypothesis** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
   - Essential for statistical significance testing
   - Widely used in research and industry

### **HIGH PRIORITY (Very Important)**
5. **Confidence Interval (C.I) Introduction & Computing** ‚≠ê‚≠ê‚≠ê‚≠ê
   - Important for uncertainty quantification
   - Used in model performance evaluation

6. **Pearson Correlation Coefficient** ‚≠ê‚≠ê‚≠ê‚≠ê
   - Feature selection and relationship analysis
   - Prerequisite for many ML concepts

7. **Correlation vs Causation** ‚≠ê‚≠ê‚≠ê‚≠ê
   - Critical thinking skill for data scientists
   - Prevents misinterpretation of results

### **MODERATE PRIORITY (Important but Specialized)**
8. **Confidence Interval using Bootstrapping** ‚≠ê‚≠ê‚≠ê
   - Advanced technique for non-parametric CI
   - Useful when distribution assumptions fail

9. **Resampling and Permutation Test** ‚≠ê‚≠ê‚≠ê
   - Non-parametric statistical testing
   - Useful for complex scenarios

10. **Kolmogorov-Smirnov (KS) Test** ‚≠ê‚≠ê‚≠ê
    - Distribution comparison technique
    - Specialized but important for data validation

### **LOWER PRIORITY (Specialized Applications)**
11. **Co-variance** ‚≠ê‚≠ê
    - Important but often covered implicitly with correlation
    - Foundation concept but limited direct application

12. **Spearman Rank Correlation Coefficient** ‚≠ê‚≠ê
    - Specialized for non-linear relationships
    - Less frequently used than Pearson

13. **Proportional Sampling** ‚≠ê‚≠ê
    - Specific sampling technique
    - Important for survey design but limited ML application

---

## Detailed Study Notes

### 1. Population and Sample üéØ

**Definition:**
- **Population**: Complete set of all possible observations
- **Sample**: Subset of population used for analysis

**Key Concepts:**
- **Parameter**: Numerical characteristic of population (Œº, œÉ)
- **Statistic**: Numerical characteristic of sample (xÃÑ, s)
- **Sampling Bias**: When sample doesn't represent population

```
Population (N) ‚Üí Sample (n) ‚Üí Inference ‚Üí Generalization
     ‚Üì              ‚Üì           ‚Üì            ‚Üì
Parameters      Statistics   Estimates    Conclusions
```

**Types of Sampling:**
- Random Sampling
- Stratified Sampling
- Cluster Sampling
- Systematic Sampling

---

### 2. Gaussian/Normal Distribution üìä

**Mathematical Definition:**
- **PDF**: f(x) = (1/‚àö(2œÄœÉ¬≤)) √ó e^(-(x-Œº)¬≤/2œÉ¬≤)
- **CDF**: Œ¶(x) = ‚à´_{-‚àû}^x f(t)dt

**Properties:**
- Mean = Median = Mode = Œº
- 68-95-99.7 Rule (Empirical Rule)
- Symmetric about mean
- Total area under curve = 1

**Standard Normal Distribution:**
- Œº = 0, œÉ = 1
- Z = (X - Œº)/œÉ (Standardization)

```mermaid
graph TD
    A[Raw Data X] --> B[Standardize: Z = X-Œº/œÉ]
    B --> C[Standard Normal N0,1]
    C --> D[Use Z-table]
    D --> E[Find Probabilities]
```

**ML Applications:**
- Linear Regression assumptions
- Naive Bayes classifier
- Central Limit Theorem
- Error distributions

---

### 3. Correlation Analysis üîó

#### Pearson Correlation Coefficient (r)

**Formula:** r = Œ£[(xi - xÃÑ)(yi - »≥)] / ‚àö[Œ£(xi - xÃÑ)¬≤Œ£(yi - »≥)¬≤]

**Properties:**
- Range: -1 ‚â§ r ‚â§ 1
- r = 1: Perfect positive correlation
- r = -1: Perfect negative correlation
- r = 0: No linear correlation

#### Spearman Rank Correlation (œÅ)

**Formula:** œÅ = 1 - (6Œ£di¬≤)/(n(n¬≤-1))

**When to Use:**
- Non-linear relationships
- Ordinal data
- Presence of outliers

```mermaid
flowchart TD
    A[Data Types] --> B{Continuous & Linear?}
    B -->|Yes| C[Pearson Correlation]
    B -->|No| D[Spearman Correlation]
    C --> E[Measures Linear Relationship]
    D --> F[Measures Monotonic Relationship]
```

#### Covariance

**Formula:** Cov(X,Y) = E[(X-Œºx)(Y-Œºy)]

**Relationship:** r = Cov(X,Y)/(œÉx √ó œÉy)

---

### 4. Correlation vs Causation ‚öñÔ∏è

**Key Principle:** Correlation ‚â† Causation

**Common Fallacies:**
1. **Post Hoc Fallacy**: A before B doesn't mean A causes B
2. **Confounding Variables**: Hidden factors affecting both variables
3. **Reverse Causation**: B might cause A instead of A causing B

```mermaid
graph LR
    A[Variable A] -.->|Correlation| B[Variable B]
    C[Hidden Variable C] --> A
    C --> B
    style C fill:#ff9999
    style A fill:#99ccff
    style B fill:#99ccff
```

**Establishing Causation:**
- Randomized Controlled Trials
- Natural Experiments
- Instrumental Variables
- Causal Inference Methods

---

### 5. Confidence Intervals üìà

#### Basic Concept

**Definition:** Range of values likely to contain the true population parameter

**General Form:** Estimate ¬± (Critical Value √ó Standard Error)

#### CI for Mean (Known œÉ)

**Formula:** xÃÑ ¬± z_(Œ±/2) √ó (œÉ/‚àön)

**Process:**
1. Choose confidence level (90%, 95%, 99%)
2. Find critical value from Z-table
3. Calculate margin of error
4. Construct interval

```mermaid
flowchart TD
    A[Sample Data] --> B[Calculate Sample Mean xÃÑ]
    B --> C[Choose Confidence Level Œ±]
    C --> D[Find Critical Value z_Œ±/2]
    D --> E[Calculate Standard Error œÉ/‚àön]
    E --> F[CI = xÃÑ ¬± z_Œ±/2 √ó SE]
```

#### CI using Bootstrapping

**Process:**
1. Resample with replacement (B times)
2. Calculate statistic for each resample
3. Sort the B statistics
4. Use percentiles for CI bounds

**Advantages:**
- No distributional assumptions
- Works for complex statistics
- Robust method

---

### 6. Hypothesis Testing üî¨

#### Methodology Framework

```mermaid
flowchart TD
    A[State Hypotheses] --> B[Choose Significance Level Œ±]
    B --> C[Select Test Statistic]
    C --> D[Calculate Test Statistic]
    D --> E[Find p-value]
    E --> F{p-value < Œ±?}
    F -->|Yes| G[Reject H‚ÇÄ]
    F -->|No| H[Fail to Reject H‚ÇÄ]
```

#### Key Components

**1. Null Hypothesis (H‚ÇÄ):**
- Statement of no effect/difference
- Assumes status quo
- What we test against

**2. Alternative Hypothesis (H‚ÇÅ):**
- Statement we want to prove
- Can be one-tailed or two-tailed

**3. p-value:**
- Probability of observing data as extreme as observed, given H‚ÇÄ is true
- Lower p-value = stronger evidence against H‚ÇÄ

#### Coin Toss Example

**Scenario:** Is a coin fair?
- H‚ÇÄ: p = 0.5 (coin is fair)
- H‚ÇÅ: p ‚â† 0.5 (coin is biased)
- Test: Flip coin 100 times, observe 65 heads
- Calculate: p-value using binomial test
- Decision: If p-value < 0.05, reject H‚ÇÄ

---

### 7. Advanced Testing Methods üß™

#### Resampling and Permutation Tests

**Permutation Test Process:**
1. Combine all observations
2. Randomly reassign group labels
3. Calculate test statistic
4. Repeat many times
5. Compare original statistic to permutation distribution

```mermaid
graph TD
    A[Original Data] --> B[Combine Groups]
    B --> C[Random Permutation]
    C --> D[Calculate Test Statistic]
    D --> E{Repeat N times}
    E -->|Yes| C
    E -->|No| F[Build Null Distribution]
    F --> G[Compare Original Statistic]
    G --> H[Calculate p-value]
```

#### Kolmogorov-Smirnov Test

**Purpose:** Test if two samples come from same distribution

**Test Statistic:** D = max|F‚ÇÅ(x) - F‚ÇÇ(x)|

**Applications:**
- Distribution comparison
- Goodness-of-fit testing
- Data validation

#### Proportional Sampling

**Concept:** Sample size from each stratum proportional to stratum size

**Formula:** n·µ¢ = n √ó (N·µ¢/N)

**Advantages:**
- Maintains population proportions
- Simple to implement
- Unbiased estimates

---

## Study Strategy üìö

### High Priority Focus (80% study time):
1. Master normal distribution properties and applications
2. Understand hypothesis testing framework completely
3. Practice correlation analysis and interpretation
4. Learn confidence interval construction

### Moderate Priority (15% study time):
1. Bootstrap methods for robust inference
2. Non-parametric testing approaches
3. Advanced correlation measures

### Lower Priority (5% study time):
1. Specialized sampling techniques
2. Distribution comparison tests

---

## Practice Problems Framework üí°

1. **Distribution Problems:**
   - Calculate probabilities using normal distribution
   - Standardization exercises
   - Real-world applications

2. **Correlation Analysis:**
   - Calculate different correlation measures
   - Interpret correlation matrices
   - Identify causation fallacies

3. **Hypothesis Testing:**
   - Design appropriate tests
   - Interpret p-values correctly
   - Avoid common statistical errors

4. **Confidence Intervals:**
   - Construct CIs for different parameters
   - Bootstrap implementation
   - Interpretation of interval meanings

---

## Key Formulas Summary üìã

| Concept | Formula | Notes |
|---------|---------|-------|
| Standard Normal | Z = (X-Œº)/œÉ | Standardization |
| Pearson r | r = Œ£[(xi-xÃÑ)(yi-»≥)]/‚àö[Œ£(xi-xÃÑ)¬≤Œ£(yi-»≥)¬≤] | Linear correlation |
| CI for mean | xÃÑ ¬± z_(Œ±/2) √ó (œÉ/‚àön) | Known population œÉ |
| t-statistic | t = (xÃÑ-Œº)/(s/‚àön) | Unknown population œÉ |
| KS statistic | D = max\|F‚ÇÅ(x) - F‚ÇÇ(x)\| | Distribution comparison |

Remember: Understanding concepts > Memorizing formulas!
